{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-24T13:49:42.821616Z",
     "start_time": "2021-11-24T13:49:33.644971Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\yosef\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import nltk\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from enchant.checker import SpellChecker\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "import swifter\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-24T13:51:18.780177Z",
     "start_time": "2021-11-24T13:49:42.856622Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80a19bc5f91b4f58a770a1d700ac2aac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79ef412fbe254f129857b9d353fe49b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index       0\n",
      "Text        0\n",
      "oh_label    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv(\"../final_data.csv\",index_col=0)\n",
    "df = df.reset_index()\n",
    "printable = set(string.printable)#ascii chars\n",
    "df['Text'] = df['Text'].swifter.apply(lambda s : ''.join(filter(lambda x: x in printable, str(s))))\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "df['Text'] = df['Text'].swifter.apply(lambda s : \" \".join([lemmatizer.lemmatize(w) for w in nltk.word_tokenize(str(s))]))\n",
    "print(df.isna().sum())\n",
    "df = df.dropna(axis='index')\n",
    "count_vect = CountVectorizer(min_df=0.01)#max_features=1000\n",
    "X_train_counts = count_vect.fit_transform(df['Text'])\n",
    "count_vect_df = pd.DataFrame(X_train_counts.toarray(), columns=count_vect.get_feature_names())\n",
    "X,y = count_vect_df,df['oh_label']\n",
    "X.shape,y.shape\n",
    "stop = set(stopwords.words('english'))\n",
    "removed_cols = []\n",
    "for col in X.columns:\n",
    "    if col in stop:\n",
    "        removed_cols.append(col)\n",
    "X = X.drop(removed_cols,axis=1)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-24T13:58:54.599902Z",
     "start_time": "2021-11-24T13:58:54.565943Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import math\n",
    "\n",
    "tf.disable_v2_behavior()\n",
    "eps = 0.2 #1e-2\n",
    "\n",
    "def save_loss(data: list, filename: str):\n",
    "    with open(filename, 'w') as f:\n",
    "        for item in data:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, X_train: np.ndarray, y_train: np.ndarray,x_test=None,y_test=None, epoch=1000, learning_rate=0.0001, batch_size=200):\n",
    "        \"\"\"\n",
    "        weighted logistic regression using cross entropy loss function\n",
    "        :param num_iter:\n",
    "        :param batch_size: -1 means all\n",
    "        \"\"\"\n",
    "        self.losses = []\n",
    "        self.test_loss = []\n",
    "        self.sess = tf.Session()\n",
    "        lr = learning_rate\n",
    "        learning_rate_tensor = tf.placeholder(tf.float64, shape=[])  # tensor for implement dynamic learning rate \n",
    "        features = X_train.shape[1]\n",
    "        self.x = tf.placeholder(tf.float64, [None, features])\n",
    "        y_train_variable = tf.placeholder(tf.float64, [None, 1])\n",
    "        W = tf.Variable(tf.random.uniform([features, 1],dtype=tf.float64))\n",
    "        b = tf.Variable(tf.random.uniform([1],dtype=tf.float64))\n",
    "        self.y = tf.add(tf.matmul(self.x, W), b)\n",
    "        w1_weight = (y_train == 1).sum()    # for imbalance data\n",
    "        w0_weight = (y_train == 0).sum()\n",
    "        # pos_weight multiple the 1 label => targets * -log(sigmoid(logits)) * pos_weight \n",
    "        loss = tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(labels= y_train_variable, logits= tf.cast(self.y, tf.float64),pos_weight=tf.constant((w1_weight+w0_weight)/w1_weight, tf.float64)))\n",
    "        update = tf.train.GradientDescentOptimizer(learning_rate = learning_rate_tensor).minimize(loss) \n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        rows_num = X_train.shape[0]\n",
    "        for i in range(0, epoch * (rows_num//batch_size)):\n",
    "            counter_step = i % (rows_num // batch_size)\n",
    "            X_batch = X_train[counter_step * batch_size:min((counter_step + 1) * batch_size, rows_num)]\n",
    "            Y_batch = y_train[counter_step * batch_size:min((counter_step + 1) * batch_size, rows_num)]\n",
    "            Y_batch = Y_batch.reshape((Y_batch.size, 1))\n",
    "            self.sess.run(update, feed_dict={self.x: X_batch, y_train_variable: Y_batch,learning_rate_tensor:lr})\n",
    "            self.sess.run(loss, feed_dict={self.x: X_batch, y_train_variable: Y_batch,learning_rate_tensor:lr})\n",
    "            loss_value = self.sess.run(loss, feed_dict={self.x: X_batch, y_train_variable: Y_batch,learning_rate_tensor:lr})\n",
    "            loss_value_test = self.sess.run(loss, feed_dict={self.x:x_test, y_train_variable: y_test.reshape(y_test.size,1),learning_rate_tensor:lr})\n",
    "            if i % (rows_num//batch_size) == 0:\n",
    "                index = i//(rows_num//batch_size) \n",
    "                print(f\"The learning rate is: {lr}\")\n",
    "                print(f\"iteration {index}: loss value is: {loss_value}\")\n",
    "                self.losses.append(loss_value)\n",
    "                self.test_loss.append(loss_value_test)\n",
    "                if  self.losses[index] > self.losses[index-1]:  # check if the loss divergence\n",
    "                    print(\"=====change learning rate=========\")\n",
    "                    lr = lr/10\n",
    "        save_loss(self.losses, filename=\"train_error.txt\")\n",
    "        save_loss(self.test_loss, filename=\"test_error.txt\")\n",
    "\n",
    "    def predict(self, X_test, thr=0.5):\n",
    "        predictions = self.sess.run(self.y, feed_dict={self.x: X_test})\n",
    "        predictions[predictions >= thr] = 1\n",
    "        predictions[predictions < thr] = 0\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-24T13:52:24.524487Z",
     "start_time": "2021-11-24T13:52:24.296712Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.shuffle(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-24T13:52:24.952878Z",
     "start_time": "2021-11-24T13:52:24.809831Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-24T13:10:50.762746Z",
     "start_time": "2021-11-24T13:10:50.744752Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((45000, 394), (5000, 394), (45000,), (5000,))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape,x_test.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-24T12:37:02.915624Z",
     "start_time": "2021-11-24T12:37:02.893458Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.940657531023937, 1.515289268721399)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_0_weight = y.size / (y == 0).sum()\n",
    "label_1_weight = y.size / (y == 1).sum()\n",
    "label_1_weight,label_0_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-24T12:32:47.514068Z",
     "start_time": "2021-11-24T12:32:45.486Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "logmodel = SGDClassifier(max_iter=100,loss='log', class_weight= {0: label_0_weight,1:label_1_weight}, random_state=42)\n",
    "logmodel.fit(x_train,y_train)\n",
    "predictions = logmodel.predict(x_test)\n",
    "print(classification_report(y_test,predictions,zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-24T12:05:16.778396Z",
     "start_time": "2021-11-24T12:04:13.943503Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The learning rate is: 0.1\n",
      "iteration 0: loss value is: 61.812302624218546\n",
      "The learning rate is: 0.1\n",
      "iteration 1: loss value is: 2.7641609092021535\n",
      "The learning rate is: 0.1\n",
      "iteration 2: loss value is: 1.6822946226275817\n",
      "The learning rate is: 0.1\n",
      "iteration 3: loss value is: 1.166243338437348\n",
      "The learning rate is: 0.1\n",
      "iteration 4: loss value is: 0.8690796976126022\n",
      "The learning rate is: 0.1\n",
      "iteration 5: loss value is: 0.6862190246422109\n",
      "The learning rate is: 0.1\n",
      "iteration 6: loss value is: 0.6216312953400823\n",
      "The learning rate is: 0.1\n",
      "iteration 7: loss value is: 0.5699115946706943\n",
      "The learning rate is: 0.1\n",
      "iteration 8: loss value is: 0.5880063413929263\n",
      "=====change learning rate=========\n",
      "The learning rate is: 0.01\n",
      "iteration 9: loss value is: 0.5950390073199796\n",
      "=====change learning rate=========\n",
      "The learning rate is: 0.001\n",
      "iteration 10: loss value is: 0.5971840802767289\n",
      "=====change learning rate=========\n",
      "The learning rate is: 0.0001\n",
      "iteration 11: loss value is: 0.5974124102114153\n",
      "=====change learning rate=========\n",
      "The learning rate is: 1e-05\n",
      "iteration 12: loss value is: 0.5974353692172876\n",
      "=====change learning rate=========\n",
      "The learning rate is: 1.0000000000000002e-06\n",
      "iteration 13: loss value is: 0.5974376663664512\n",
      "=====change learning rate=========\n",
      "The learning rate is: 1.0000000000000002e-07\n",
      "iteration 14: loss value is: 0.5974378960938417\n",
      "=====change learning rate=========\n",
      "The learning rate is: 1.0000000000000002e-08\n",
      "iteration 15: loss value is: 0.5974379190667052\n",
      "=====change learning rate=========\n",
      "The learning rate is: 1.0000000000000003e-09\n",
      "iteration 16: loss value is: 0.5974379213639931\n",
      "=====change learning rate=========\n",
      "The learning rate is: 1.0000000000000003e-10\n",
      "iteration 17: loss value is: 0.5974379215937216\n",
      "=====change learning rate=========\n",
      "The learning rate is: 1.0000000000000003e-11\n",
      "iteration 18: loss value is: 0.5974379216166947\n",
      "=====change learning rate=========\n",
      "The learning rate is: 1.0000000000000002e-12\n",
      "iteration 19: loss value is: 0.5974379216189922\n",
      "=====change learning rate=========\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.79      0.83      3209\n",
      "         1.0       0.66      0.78      0.72      1676\n",
      "\n",
      "    accuracy                           0.79      4885\n",
      "   macro avg       0.77      0.79      0.78      4885\n",
      "weighted avg       0.80      0.79      0.79      4885\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logistic = LogisticRegression(x_train,y_train.values,x_test,y_test.values,epoch=20,learning_rate=0.1,batch_size=150)\n",
    "predictions = logistic.predict(x_test)\n",
    "print(classification_report(y_test, predictions,zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-24T12:05:24.490911Z",
     "start_time": "2021-11-24T12:05:24.368420Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x28be3d0f550>]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "data_train = np.loadtxt(\"train_error.txt\")\n",
    "data_test = np.loadtxt(\"test_error.txt\")\n",
    "plt.plot(data_train,color='g')\n",
    "plt.plot(data_test,color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-24T12:37:07.705136Z",
     "start_time": "2021-11-24T12:37:07.695125Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-24T21:42:48.215517Z",
     "start_time": "2021-11-24T21:42:48.174905Z"
    }
   },
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \"\"\"\n",
    "    multi level perceptron implementation using tensorflow version 1\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self, x_train: np.ndarray, y_train, layers_sizes=(100,), learning_rate=0.1, epoch=50, batch_size=100):\n",
    "        \"\"\"\n",
    "        Feed Foreword Neural network using Batch gradient decent optimizer\n",
    "        :param layers_sizes: len of this list need to be greater than 1\n",
    "        :param num_iter:\n",
    "        :param print_step: print loss value every print_step echos\n",
    "        \"\"\"\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.1, random_state=1)\n",
    "        print(x_train.shape,x_test.shape,y_test.shape)\n",
    "        self.sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=4))\n",
    "        self.losses = []\n",
    "        self.test_loss = []\n",
    "        lr = learning_rate\n",
    "        learning_rate_tensor = tf.placeholder(tf.float64, shape=[])  # tensor for implement dynamic learning rate \n",
    "        rows_num, features = x_train.shape[0], x_train.shape[1]\n",
    "        self.x = tf.placeholder(tf.float64, [None, features])\n",
    "        y_train_variable = tf.placeholder(tf.float64, [None, 1])\n",
    "        layers_sizes = [features] + list(layers_sizes) + [1]\n",
    "        W, b = [], []\n",
    "        for i, layer_size in enumerate(layers_sizes[1:]):\n",
    "            W.append(tf.Variable(tf.random.uniform([layers_sizes[i], layer_size],dtype=tf.float64)))\n",
    "            b.append(tf.Variable(tf.random.uniform([layer_size],dtype=tf.float64)))\n",
    "        # ff\n",
    "        prev_output = tf.nn.relu(tf.matmul(self.x, W[0]) + b[0])\n",
    "        for layer_w, layer_b in zip(W[1:-1], b[1:-1]):\n",
    "            print(prev_output)\n",
    "            prev_output = tf.nn.relu(tf.add(tf.matmul(prev_output, layer_w), layer_b))\n",
    "        print(prev_output)\n",
    "        self.y = tf.add(tf.matmul(prev_output, W[-1]), b[-1])\n",
    "        print(self.y)\n",
    "        w1_weight = (y_train == 1).sum()    # for imbalance data\n",
    "        w0_weight = (y_train == 0).sum()\n",
    "        # pos_weight multiple the 1 label => targets * -log(sigmoid(logits)) * pos_weight \n",
    "        loss = tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(labels=y_train_variable, logits= tf.cast(self.y, tf.float64),pos_weight=tf.constant((w1_weight+w0_weight)/w1_weight, tf.float64)))\n",
    "        update = tf.train.GradientDescentOptimizer(learning_rate = learning_rate_tensor).minimize(loss) \n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        rows_num = x_train.shape[0]\n",
    "        for i in range(0, epoch * (rows_num//batch_size)):\n",
    "            counter_step = i % (rows_num // batch_size)\n",
    "            X_batch = x_train[counter_step * batch_size:min((counter_step + 1) * batch_size, rows_num)]\n",
    "            Y_batch = y_train[counter_step * batch_size:min((counter_step + 1) * batch_size, rows_num)]\n",
    "            Y_batch = Y_batch.reshape((Y_batch.size, 1))\n",
    "            self.sess.run(update, feed_dict={self.x: X_batch, y_train_variable: Y_batch,learning_rate_tensor:lr})\n",
    "            loss_value = self.sess.run(loss, feed_dict={self.x: X_batch, y_train_variable: Y_batch,learning_rate_tensor:lr})\n",
    "            loss_value_test = self.sess.run(loss, feed_dict={self.x:x_test, y_train_variable: y_test.reshape(y_test.size,1),learning_rate_tensor:lr})\n",
    "            if i % (rows_num//batch_size) == 0:\n",
    "                index = i//(rows_num//batch_size) \n",
    "                print(f\"The learning rate is: {lr}\")\n",
    "                print(f\"iteration {index}: loss value is: {loss_value}\")\n",
    "                self.losses.append(loss_value)\n",
    "                self.test_loss.append(loss_value_test)\n",
    "                if  self.losses[index] > self.losses[index-1]:  # check if the loss divergence\n",
    "                    print(\"=====change learning rate=========\")\n",
    "                    lr = lr/10\n",
    "        save_loss(self.losses, filename=\"MLP_train_error.txt\")\n",
    "        save_loss(self.test_loss, filename=\"MLP_test_error.txt\")\n",
    "\n",
    "    def predict(self, X_test, thr=0.5):\n",
    "        predictions = self.sess.run(tf.nn.sigmoid(self.sess.run(self.y, feed_dict={self.x: X_test})))\n",
    "        predictions[predictions >= thr] = 1\n",
    "        predictions[predictions < thr] = 0\n",
    "        return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-11-24T21:42:48.565Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40500, 394) (4500, 394) (4500,)\n",
      "Tensor(\"Relu_18:0\", shape=(?, 200), dtype=float64)\n",
      "Tensor(\"Relu_19:0\", shape=(?, 100), dtype=float64)\n",
      "Tensor(\"Add_29:0\", shape=(?, 1), dtype=float64)\n",
      "The learning rate is: 0.1\n",
      "iteration 0: loss value is: 4788580.263123191\n",
      "The learning rate is: 0.1\n",
      "iteration 1: loss value is: 1.1027967967606358\n",
      "The learning rate is: 0.1\n",
      "iteration 2: loss value is: 1.1025815617833887\n",
      "The learning rate is: 0.1\n",
      "iteration 3: loss value is: 1.1025537990922563\n",
      "The learning rate is: 0.1\n",
      "iteration 4: loss value is: 1.102553143810504\n",
      "The learning rate is: 0.1\n",
      "iteration 5: loss value is: 1.1025531436299227\n",
      "The learning rate is: 0.1\n",
      "iteration 6: loss value is: 1.1025531436316616\n",
      "=====change learning rate=========\n",
      "The learning rate is: 0.01\n",
      "iteration 7: loss value is: 1.1023848287896305\n",
      "The learning rate is: 0.01\n",
      "iteration 8: loss value is: 1.1023138974716902\n",
      "The learning rate is: 0.01\n",
      "iteration 9: loss value is: 1.1022830263827246\n",
      "The learning rate is: 0.01\n",
      "iteration 10: loss value is: 1.102269380964063\n",
      "The learning rate is: 0.01\n",
      "iteration 11: loss value is: 1.1022633061930187\n",
      "The learning rate is: 0.01\n",
      "iteration 12: loss value is: 1.1022605929710765\n",
      "The learning rate is: 0.01\n",
      "iteration 13: loss value is: 1.102259379366028\n",
      "The learning rate is: 0.01\n",
      "iteration 14: loss value is: 1.10225883617156\n",
      "The learning rate is: 0.01\n",
      "iteration 15: loss value is: 1.1022585929727624\n",
      "The learning rate is: 0.01\n",
      "iteration 16: loss value is: 1.1022584840735516\n",
      "The learning rate is: 0.01\n",
      "iteration 17: loss value is: 1.1022584353079872\n",
      "The learning rate is: 0.01\n",
      "iteration 18: loss value is: 1.1022584134700202\n",
      "The learning rate is: 0.01\n",
      "iteration 19: loss value is: 1.1022584036905858\n",
      "The learning rate is: 0.01\n",
      "iteration 20: loss value is: 1.1022583993112074\n",
      "The learning rate is: 0.01\n",
      "iteration 21: loss value is: 1.1022583973501057\n",
      "The learning rate is: 0.01\n",
      "iteration 22: loss value is: 1.1022583964719683\n",
      "The learning rate is: 0.01\n",
      "iteration 23: loss value is: 1.1022583960788068\n",
      "The learning rate is: 0.01\n",
      "iteration 24: loss value is: 1.1022583959028307\n",
      "The learning rate is: 0.01\n",
      "iteration 25: loss value is: 1.1022583958241148\n",
      "The learning rate is: 0.01\n",
      "iteration 26: loss value is: 1.1022583957889507\n",
      "The learning rate is: 0.01\n",
      "iteration 27: loss value is: 1.1022583957732877\n",
      "The learning rate is: 0.01\n",
      "iteration 28: loss value is: 1.1022583957663599\n",
      "The learning rate is: 0.01\n",
      "iteration 29: loss value is: 1.1022583957633385\n",
      "The learning rate is: 0.01\n",
      "iteration 30: loss value is: 1.1022583957620677\n",
      "The learning rate is: 0.01\n",
      "iteration 31: loss value is: 1.102258395761576\n",
      "The learning rate is: 0.01\n",
      "iteration 32: loss value is: 1.1022583957614338\n",
      "The learning rate is: 0.01\n",
      "iteration 33: loss value is: 1.102258395761446\n",
      "=====change learning rate=========\n",
      "The learning rate is: 0.001\n",
      "iteration 34: loss value is: 1.1022565889589275\n",
      "The learning rate is: 0.001\n",
      "iteration 35: loss value is: 1.1022549231574397\n",
      "The learning rate is: 0.001\n",
      "iteration 36: loss value is: 1.102253387229417\n",
      "The learning rate is: 0.001\n",
      "iteration 37: loss value is: 1.1022519709435894\n",
      "The learning rate is: 0.001\n",
      "iteration 38: loss value is: 1.1022506648901738\n",
      "The learning rate is: 0.001\n",
      "iteration 39: loss value is: 1.1022494604126707\n",
      "The learning rate is: 0.001\n",
      "iteration 40: loss value is: 1.1022483495456499\n",
      "The learning rate is: 0.001\n",
      "iteration 41: loss value is: 1.1022473249579312\n",
      "The learning rate is: 0.001\n",
      "iteration 42: loss value is: 1.1022463799006974\n"
     ]
    }
   ],
   "source": [
    "logistic = MLP(x_train,y_train.values,layers_sizes=(200,100),epoch=140,batch_size=200,learning_rate=0.1)\n",
    "predictions = logistic.predict(x_test)\n",
    "print(classification_report(y_test, predictions,zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
